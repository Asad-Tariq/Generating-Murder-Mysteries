# CSS-201S/202S - Introduction to Computational Social Science Technical Bootcamp

## Final Project

### Generating Murder Mysteries using an open source dataset.

#### Description
My project utilizes a Transformer model, which is known for its self attention mechanism and ability to generate natural-sounding text. The model is trained on a corpus of murder mystery novels by Agatha Christie and is used for generating a new murder mystery.

#### Features
1. Input Layer:
    - Layer type: InputLayer
    - Shape: (None, None)
    - Takes in input sequences with an unspecified length, allowing the model to handle different lengths of text or sequence inputs.

2. Token and Position Embedding Layer:
    - Layer type: TokenAndPositionEmbedding
    - Shape: (None, None, 256)
    - Parameters: 4,096,000
    - Responsible for converting input tokens (words or subword units) into vector representations of size 256. It combines token embeddings (turning each word into a dense vector) and positional embeddings (adding information about the position of each word in the sequence).

3. Transformer Block:
   - Layer type: TransformerBlock
   - Shape: [(None, None, 256), (None, 4, None, None)]
   - Parameters: 1,184,512
   - The Transformer block is the core part of the model. It includes:
      - Self-attention Mechanism: Allows the model to weigh the importance of each word in the sequence when processing a word.
      - Multi-head Attention: This layer uses multi-head attention with 4 attention heads which allows the model to focus on different parts of the sequence simultaneously.
      - Feedforward Network: The Transformer block also contains a position-wise feedforward network that processes the output from the attention mechanism.

4. Dense Layer:
   - Layer type: Dense
   - Shape: (None, None, 11000)
   - Parameters: 2,827,000
   - A fully connected (dense) layer that outputs a vector of size 11,000 for each input word. This corresponds to a vocabulary size of 11,000, meaning the model predicts one of 11,000 possible tokens at each step (e.g., words or subwords).
   - This layer is typically used at the end of the model to project the learned embeddings back into a vocabulary space for predictions (e.g., generating the next word in a sentence).

#### Usage
1. Open the notebook and run all cells to generate a unique murder mystery plot.
2. Customizable Inputs: You can customize the number of characters, the relationships between them, and the complexity of the plot by adjusting input prompts in the notebook.
3. Execution: Each time you run the notebook, it generates a new murder mystery with a different set of characters, clues, motives, and a final twist.

#### File and Directory Structure
- <tt> generating_murder_mysteries.ipynb: </tt> The main notebook containing the code for generating murder mystery plots using a Transformer model.
- <tt> README.md: </tt> (This file) A guide providing instructions and details about the project.
- <tt> corpus: </tt>: Directory containing the murder mystery novel corpus on which the Transformer model is trained.
- <tt> generated_texts: </tt> Directory which has the file containing the contents generated by the Transformer model.
- <tt> models: </tt> Directory which the **user** will have to create if they wish to save or load the Transformer model locally.

#### Contributing

Contributions are welcome! If you have ideas for additional features (e.g., new plot twists or character traits), bug fixes, or enhancements, feel free to submit a pull request or open an issue.

#### License

This project is licensed under the MIT License - see the LICENSE file for details.